% Description: Contains the software state of the art.

\section{Software}
En esta sección, hablaremos de todos los temas relacionados con software que se han investigado para el desarrollo de este proyecto. Se abordarán temas como los firmwares de los controladores de vuelo, las estaciones de control en tierra, los sistemas operativos de las computadoras auxiliares, documentación de ROS 2, visión por computadora, marcadores ArUco, entre otros.

\subsection{FC Firmware}
    La definicion de firmware según ... es \textit{} \cite{}, en el caso de los controladores de vuelo, el firmware es el software que se ejecuta en el controlador de vuelo y se encarga de gestionar los sensores, actuadores y algoritmos de control necesarios para mantener la estabilidad y control del vehículo. Existen varios firmwares de controladores de vuelo disponibles en la actualidad, cada uno con sus propias características y ventajas. En este proyecto, se han investigado dos de los firmwares más populares y ampliamente utilizados en la industria de los drones y vehículos no tripulados: PX4 y ArduPilot.
    \subsubsection{Firmware PX4}
    El firmware PX4 ha sido una pieza fundamental en el desarrollo de sistemas de control de vuelo para drones y vehículos no tripulados, destacándose por su naturaleza de código abierto y su flexibilidad para muchas aplicaciones. Según la documentación oficial, \textit{``PX4 is a powerful open source autopilot flight stack running on the NuttX RTOS''} \cite{px4_docs}. Este sistema es ampliamente reconocido por su arquitectura modular, que permite la integración de nuevos sensores, actuadores y algoritmos de control, facilitando adaptaciones específicas para cumplir con los requisitos de diferentes proyectos \cite{px4_docs}.

    Entre sus características más destacadas, se encuentra su capacidad para soportar una amplia variedad de tipos de vehículos. En este sentido, PX4 \textit{``supports many different vehicle frames/types, including: multicopters, fixed-wing aircraft (planes), VTOLs (hybrid multicopter/fixed-wing), ground vehicles, and underwater vehicles''} \cite{px4_docs}. Esta capacidad de soportar múltiples configuraciones es esencial, ya que permite aplicar el mismo marco de trabajo a diversas plataformas con modificaciones mínimas.

    Además, PX4 es parte integral de un ecosistema más amplio que incluye estaciones de control en tierra como QGroundControl, Mission Planner y hardware específico como Pixhawk. La documentación menciona que \textit{``PX4 is a core part of a broader drone platform that includes the QGroundControl ground station, Pixhawk hardware, and MAVSDK for integration with companion computers, cameras, and other hardware using the MAVLink protocol''} \cite{px4_docs}. Esta integración asegura una comunicación fluida entre los diferentes componentes del sistema, lo cual es crucial para el monitoreo en tiempo real y la planificación de misiones.

    Finalmente, PX4 ofrece \textit{``flexible and powerful flight modes and safety features''} que son vitales para proyectos que requieren maniobras complejas, como el aterrizaje autónomo preciso en estaciones de carga \cite{px4_docs}. Estas funcionalidades avanzadas de control y su capacidad de integrarse con tecnologías de visión por computadora y sensores externos consolidan a PX4 como una herramienta clave en el desarrollo de tecnologías de alta relevancia para vehículos autónomos.
    
    \subsubsection{Firmware ArduPilot}
    ArduPilot, al igual que PX4, es un software de código abierto que se ejecuta en una amplia variedad de hardware, permitiendo la creación y uso de sistemas autónomos de vehículos no tripulados para diferentes aplicaciones. Según la documentación oficial, \textit{“ArduPilot provides a comprehensive suite of tools suitable for almost any vehicle and application. As an open source project, it is constantly evolving based on rapid feedback from a large community of users”} \cite{ardupilot_docs}. Esta flexibilidad y capacidad de adaptación han convertido a ArduPilot en una herramienta esencial para proyectos de automatización y robótica que buscan una solución versátil y robusta.

    Uno de los aspectos más destacados de ArduPilot es que, a diferencia de PX4, éste no fabrica hardware, por lo cual \textit{“ArduPilot firmware works on a wide variety of different hardware to control unmanned vehicles of all types”} \cite{ardupilot_docs}. Esto significa que puede integrarse con distintos tipos de controladores, sensores y dispositivos, transformando prácticamente cualquier máquina móvil en un vehículo autónomo con la simple adición de un paquete de hardware adecuado.

    Como se mencionó anteriormente, el firmware es el código que se ejecuta en el controlador, y la elección del tipo de firmware depende del vehículo y la misión. Como se menciona en la documentación, \textit{“You choose the firmware to match your vehicle and mission: Copter, Plane, Rover, Sub, or Antenna Tracker”} \cite{ardupilot_docs}. Esta versatilidad permite a los desarrolladores seleccionar la configuración más apropiada para sus necesidades específicas, optimizando así el proceso de desarrollo y operación.

    ArduPilot también se complementa con estaciones de control en tierra (GCS), que funcionan como la interfaz entre el usuario y el controlador del vehículo. Una de las herramientas más completas en este ámbito es Mission Planner, descrita como \textit{“a full-featured GCS supported by ArduPilot”}, que ofrece interacción fácil y rápida con el firmware al usuario \cite{ardupilot_docs}.


    Por último, mencionar que tanto PX4 como Ardupilot, tienen funcionalidades similares como la calibraicon de sensores, planificación de misiones, configuración de parámetros, entre otros. Sin embargo, la elección de uno u otro dependerá de las necesidades específicas del proyecto y de la compatibilidad con el \textbf{hardware} disponible.

\subsection{Ground Control Systems}

Las estaciones de control en tierra (GCS, por sus siglas en inglés) son herramientas auxiliares ampliamente utilizadas para la configuración, calibración y monitoreo de drones. Aunque no son el foco principal de este proyecto, desempeñan un papel importante en la preparación del sistema, facilitando la configuración de parámetros, la calibración de sensores y la supervisión durante vuelos de prueba.

Entre las herramientas utilizadas se encuentran QGroundControl y Mission Planner, que destacan por su compatibilidad con los firmwares PX4 y ArduPilot, respectivamente. Estas plataformas proporcionan interfaces intuitivas para ajustar parámetros del vehículo, planificar misiones, y visualizar datos de telemetría en tiempo real. En el contexto de este proyecto, han sido clave para garantizar que el dron esté correctamente configurado antes de integrarse con la estación de carga modular.

A pesar de su utilidad durante las etapas de preparación y pruebas, el sistema propuesto no depende directamente de estas herramientas para su funcionamiento final. La operación autónoma del dron y la estación de carga se basa en la interacción directa entre el firmware, los sistemas de visión, y los controladores de vuelo, sin necesidad de intervención continua desde una estación de control en tierra.

    \begin{figure}[h!]
        \centering
        \begin{minipage}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pictures/qgc_interface.jpg} % Imagen ilustrativa de QGroundControl
            \caption{Interfaz de QGroundControl.}
            \label{fig:qgc}
        \end{minipage}
        \hfill
        \begin{minipage}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pictures/mission_planner.jpg} % Imagen ilustrativa de Mission Planner
            \caption{Interfaz de Mission Planner.}
            \label{fig:mission_planner}
        \end{minipage}
        \caption{Estaciones de control en tierra utilizadas durante las etapas de configuración y pruebas.}
        \label{fig:gcs}
    \end{figure}


\subsection{Companion Computer Operative Systems} 
    
    Para poder correr una companion computer se necesita elegir dentro de la multiples opciones de sistemas operativos disponibles, algunos de los más populares son Ubuntu y Raspberry Pi OS. A continuación, se describen las características y ventajas de cada uno de estos sistemas operativos.

    \subsubsection{Ubuntu}

    Ubuntu es un sistema operativo de código abierto basado en Linux que ha ganado popularidad por su estabilidad y amplia comunidad de soporte, es importante saber que dentro de Ubuntu y sus versiones, las versiones pares son las que tienen mayor estabilidad. Según la documentación oficial de Ubuntu, este sistema es \textit{“designed for security, reliability, and ease of use”} \cite{ubuntu_docs}. En el contexto de los companion computers para drones y otros vehículos autónomos, Ubuntu se utiliza frecuentemente debido a su compatibilidad con herramientas de robótica como ROS (Robot Operating System), lo que facilita la integración y desarrollo de software avanzado de control y automatización.

    Ubuntu ofrece soporte para arquitecturas ARM, lo que permite su instalación y ejecución en dispositivos como la Raspberry Pi 4 y 5. Esta capacidad es fundamental para proyectos que requieren procesamiento local eficiente, manejo de datos de sensores y comunicación en tiempo real. Adicionalmente, la flexibilidad de Ubuntu permite la personalización de su entorno para adaptarse a las necesidades específicas del proyecto, ya sea para ejecutar nodos de control de vuelo o procesamiento de imágenes en tiempo real \cite{ubuntu_docs}.


    \subsubsection{Raspberry Pi OS}

    Raspberry Pi OS, es el sistema operativo oficial desarrollado y optimizado para dispositivos Raspberry Pi. La documentación de Raspberry Pi OS lo describe como \textit{“a Debian-based operating system specifically tuned for the Raspberry Pi hardware”} \cite{raspbian_docs}. Su principal ventaja es su optimización para el hardware de Raspberry Pi, lo que garantiza un rendimiento óptimo y un uso eficiente de los recursos disponibles.

    Raspbian incluye una serie de herramientas preinstaladas que facilitan el desarrollo y prototipado, haciendo que sea una opción preferida para proyectos educativos y de investigación. La compatibilidad con Python y otras bibliotecas de programación facilita la implementación de scripts y software necesarios para muchas aplicaciones de robótica.

    En comparación con otros sistemas operativos, Raspbian es ligero y permite un \textbf{arranque rápido}, lo cual es beneficioso en escenarios donde se requiere un inicio rápido del sistema.


\subsection{ROS2 Distributions}
    Las distribuciones de ROS 2 (Robot Operating System 2) proporcionan entornos estandarizados para el desarrollo de aplicaciones robóticas, cada una adaptada a diferentes necesidades y capacidades de hardware. Una de las ventajas de ROS 2 es su capacidad para \textbf{facilitar la comunicación entre distribuciones} mediante el uso de topics, lo que permite que nodos en distintas versiones de ROS 2 se comuniquen y colaboren de manera efectiva en un mismo proyecto \cite{ros_docs}. En la siguiente sección se abordarán más detalles sobre las distribuciones de ROS 2 más recientes y estables.

    \subsubsection{Distribución Humble}

    La distribución \textbf{ROS 2 Humble Hawksbill} es conocida por ser una versión con \textbf{soporte a largo plazo (LTS)}, lo que garantiza actualizaciones constantes y confiables. Esta distribución está diseñada para proyectos que requieren estabilidad y una alta compatibilidad con diferentes sistemas y paquetes. Es ideal para hardware como la \textbf{Raspberry Pi 4} y es compatible con \textbf{Ubuntu 22.04} y versiones anteriores, como Ubuntu 20.04, lo que la hace accesible para configuraciones de hardware más estándar.
    
    De acuerdo con la documentación oficial de ROS 2, Humble es una de las versiones más recomendadas para proyectos que buscan consistencia a lo largo del tiempo, debido a su enfoque en evitar cambios disruptivos \cite{humble_documentation}.
    
    \paragraph{Características principales:}  
    \begin{itemize}
        \item \textbf{LTS (Long-Term Support):} Asegura soporte prolongado para actualizaciones y corrección de errores.
        \item \textbf{Estabilidad:} Mejoras comprobadas en la comunicación entre nodos, garantizando confiabilidad.
        \item \textbf{Compatibilidad amplia:} Funciona con la mayoría de las bibliotecas y paquetes existentes en el ecosistema de ROS 2.
        \item \textbf{Optimización para ARM:} Ideal para plataformas como la Raspberry Pi 4, aprovechando recursos limitados de manera eficiente.
    \end{itemize}
    
    En general, Humble es la opción preferida para quienes priorizan estabilidad en sus proyectos de investigación o aplicaciones a largo plazo.
    
    \subsubsection{Distribución Jazzy}
    
    La distribución \textbf{ROS 2 Jazzy Jalisco}, más reciente que Humble, está diseñada para sacar ventaja del hardware más moderno, como la \textbf{Raspberry Pi 5}, y es compatible con sistemas operativos avanzados como \textbf{Ubuntu 24.04}. Esta distribución incluye mejoras significativas en rendimiento y nuevas funcionalidades, pero se considera más experimental en comparación con versiones LTS.
    
    Según la documentación de Jazzy, su principal enfoque es facilitar el desarrollo de aplicaciones robóticas que aprovechen las últimas herramientas y simuladores, además de mejorar la velocidad en la comunicación entre nodos \cite{jazzy_documentation}.
    
    \paragraph{Características principales:}  
    \begin{itemize}
        \item \textbf{Nuevas funcionalidades:} Introducción de características experimentales y ajustes de rendimiento.
        \item \textbf{Menor latencia:} Comunicación más rápida entre nodos, clave para aplicaciones en tiempo real.
        \item \textbf{Integración avanzada:} Mejoras en simulación y depuración, aprovechando herramientas como Gazebo y Rviz.
        \item \textbf{Orientación al hardware moderno:} Diseñada para dispositivos como la Raspberry Pi 5, que ofrecen mayor capacidad de procesamiento.
    \end{itemize}

    \begin{figure}[h!]
        \centering
        \begin{minipage}{0.45\textwidth}
            \centering
            \includegraphics[width=0.5\textwidth]{pictures/humble_logo.png}
            \caption{ROS 2 Humble}
            \label{fig:imagen1}
        \end{minipage}
        \hfill
        \begin{minipage}{0.45\textwidth}
            \centering
            \includegraphics[width=0.6\textwidth]{pictures/jazzy_logo.png}
            \caption{ROS 2 Jazzy Jalisco}
            \label{fig:imagen2}
        \end{minipage}
    \end{figure}

\subsection{Arquitectura de ROS 2}

    ROS 2 (Robot Operating System 2) es un conjunto de bibliotecas y herramientas diseñadas para desarrollar aplicaciones de robótica. Es la evolución de ROS 1, creada para resolver problemas relacionados con la escalabilidad, seguridad y soporte a sistemas en tiempo real. ROS 2 utiliza una arquitectura basada en el middleware \textbf{DDS (Data Distribution Service)}, que facilita la comunicación entre nodos en sistemas distribuidos, asegurando interoperabilidad y un bajo tiempo de respuesta \cite{ros_docs}.  
    
    \subsubsection{Conceptos básicos de ROS 2}
    
    ROS 2 está organizado en torno a varios elementos fundamentales que permiten la interacción entre las diferentes partes del sistema robótico. Estos elementos, que definen su arquitectura modular, son esenciales para comprender cómo se estructura y funciona un sistema basado en ROS 2 \cite{ros_docs}:  
    
    \begin{itemize}
        \item \textbf{Nodos:} Son las unidades básicas de ejecución en ROS 2. Cada nodo cumple una función específica, como recopilar datos de un sensor o controlar un actuador. Los nodos están diseñados para ser independientes y comunicarse entre sí mediante topics, servicios o acciones.  
        \item \textbf{Messages (Mensajes):} Son estructuras de datos que se envían entre nodos para compartir información. Los mensajes tienen un formato definido que asegura que todos los nodos comprendan el contenido.  
        \item \textbf{Topics:} Representan canales de comunicación para el intercambio de mensajes entre nodos. La comunicación mediante topics es asincrónica, lo que permite que los nodos publiquen y reciban información sin necesidad de esperar respuestas inmediatas.  
        \item \textbf{Servicios (Services):} Permiten la comunicación síncrona entre nodos. A diferencia de los topics, los servicios funcionan en un esquema de solicitud y respuesta, lo cual es útil para operaciones específicas.  
        \item \textbf{Actions:} Son similares a los servicios, pero están diseñados para operaciones de larga duración. Permiten a los nodos recibir actualizaciones del progreso de una tarea y cancelarla si es necesario.  
        \item \textbf{Launch Files:} Facilitan la configuración y el inicio simultáneo de varios nodos. Esto simplifica la gestión de aplicaciones complejas, especialmente en entornos distribuidos.  
        \item \textbf{Parámetros:} Son valores configurables que los nodos utilizan para ajustar su comportamiento sin necesidad de modificar el código.  
    \end{itemize}
    
    La combinación de estos elementos permite que los sistemas robóticos diseñados en ROS 2 sean altamente escalables y flexibles, adaptándose tanto a proyectos pequeños como a sistemas complejos.  
    
    % Imagen de la arquitectura de ROS 2
    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.6\textwidth]{pictures/ros2_arch.png}
        \caption{Arquitectura de ROS 2 \cite{ros_docs}.}
        \label{fig:ros2_architecture}
    \end{figure}
    
    \subsubsection{Beneficios clave de ROS 2}
    
    La arquitectura de ROS 2 trae consigo varias ventajas que la hacen ideal para sistemas robóticos distribuidos y modernos. Entre los beneficios clave se encuentran los siguientes \cite{ros_docs}:  
    
    \begin{table}[h!]
    \centering
    \caption{Beneficios clave de ROS 2}
    \begin{tabular}{|l|p{10cm}|}
    \hline
    \textbf{Beneficio} & \textbf{Descripción} \\
    \hline
    Multithreading & Permite la ejecución de nodos en paralelo, optimizando el rendimiento. \\
    \hline
    Uso de topics & Comunicación asincrónica entre nodos para el intercambio de datos. \\
    \hline
    Nodos y servicios & Los nodos interactúan mediante servicios y topics para tareas específicas. \\
    \hline
    Archivos de lanzamiento (Launch Files) & Configuran y ejecutan múltiples nodos al mismo tiempo. \\
    \hline
    Comunicación rápida & Basada en DDS, proporciona comunicación de baja latencia entre nodos. \\
    \hline
    Mensajes personalizados & Permite definir y utilizar estructuras específicas para cada aplicación. \\
    \hline
    Soporte a tiempo real & Habilita aplicaciones críticas donde el tiempo de respuesta es esencial. \\
    \hline
    \end{tabular}
    \label{table:benefits}
    \end{table}    


\subsection{Computer Vision}

    La visión por computadora es una rama de la programación que permite a los sistemas interpretar y procesar información visual del entorno. Este campo es clave en aplicaciones que requieren análisis de imágenes y videos en tiempo real, como la navegación autónoma de robots, detección de objetos y sistemas de inteligencia artificial. Para facilitar el desarrollo de estas aplicaciones, existen herramientas y bibliotecas especializadas como OpenCV, que juega un papel fundamental en la implementación práctica de visión por computadora \cite{opencv_docs}.  
    
    \subsubsection{OpenCV Library}
    
    OpenCV (Open Source Computer Vision Library) es una de las bibliotecas más utilizadas en la industria y en la academia para el desarrollo de soluciones de visión por computadora. Según su documentación oficial, OpenCV es \textit{“an open-source computer vision and machine learning software library containing more than 2500 optimized algorithms”} \cite{opencv_docs}. Estas herramientas permiten realizar tareas como procesamiento de imágenes, reconocimiento de objetos y seguimiento de movimiento, siendo compatibles con lenguajes de programación como Python y C++.  
    
    Gracias a su flexibilidad y facilidad de uso, OpenCV se adapta tanto a principiantes como a expertos. Además, sus algoritmos optimizados permiten ejecutar procesos en tiempo real, lo que la convierte en una herramienta ideal para proyectos de robótica y aplicaciones autónomas \cite{opencv_docs}.  
    
    \subsubsection{Camera Calibration}
    
    Antes de implementar cualquier sistema de visión por computadora en aplicaciones que demanden precisión espacial, es fundamental realizar una calibración adecuada de las cámaras utilizadas. Este proceso corrige las distorsiones inherentes a las lentes y permite obtener mediciones precisas del entorno. Según la documentación de OpenCV, \textit{“Camera calibration is the process of estimating the parameters of the lens and the image sensor of an imaging device”} \cite{opencv_calib3d}.  
    
    La calibración implica determinar parámetros intrínsecos (como la distancia focal y el punto principal) y extrínsecos (relativos a la posición y orientación de la cámara) que permiten mapear las coordenadas 2D de las imágenes capturadas a coordenadas 3D del mundo real.  
    
    \begin{figure}[h!] 
        \centering 
        \includegraphics[width=0.6\textwidth]{pictures/distortions.png} % Imagen de distorsiones de barril y cojín inherentes a las cámaras 
        \caption{Tipos de distorsiones en las cámaras.} 
        \label{fig:distortions} 
    \end{figure}  
    
    OpenCV proporciona funciones que permiten realizar este proceso de manera eficiente mediante la detección de patrones en imágenes, como tableros de ajedrez o arreglos de círculos. El flujo típico de calibración incluye:  
    
    \begin{itemize}
        \item Capturar imágenes de un patrón conocido desde diferentes ángulos.  
        \item Identificar puntos de interés en las imágenes capturadas (esquinas del patrón).  
        \item Usar algoritmos de optimización para calcular parámetros intrínsecos y coeficientes de distorsión.  
    \end{itemize}  
    
    El resultado de la calibración permite corregir las distorsiones en imágenes y videos, mejorando la precisión de las aplicaciones de visión por computadora. Esto es especialmente útil en proyectos que requieren análisis espacial preciso, como la navegación autónoma y el posicionamiento de objetos en tiempo real.  
    
    \begin{figure}[h!] 
        \centering 
        \includegraphics[width=0.4\textwidth]{pictures/calib_pattern.jpg} % Imagen ilustrativa de un tablero de ajedrez para calibración de cámaras 
        \caption{Patrón de tablero de ajedrez utilizado en la calibración de cámaras con OpenCV.} 
        \label{fig:calib_pattern} 
    \end{figure}  

\subsection{What is an ArUco?}
    ArUco, cuyo nombre proviene de la combinación de "Artificial" y "Uco" (por la Universidad de Córdoba, donde se desarrolló), es una biblioteca de código abierto ampliamente reconocida en el ámbito de la visión por computadora para la detección de marcadores fiduciales en imágenes. Esta tecnología es fundamental para la estimación de la pose de la cámara con respecto a los marcadores cuando la cámara ha sido previamente calibrada. De acuerdo con la documentación, \textit{“ArUco is an OpenSource library for detecting squared fiducial markers in images”} \cite{aruco_docs}. La detección de estos marcadores es crucial en aplicaciones que requieren una estimación precisa de la posición y orientación de objetos en el espacio tridimensional.

    \subsubsection{Historia de los Marcadores Aruco}

    Los marcadores ArUco se originaron como una solución para superar las limitaciones de otras tecnologías de detección de patrones, colores o figuras. El objetivo era desarrollar una técnica que proporcionara alta fiabilidad incluso bajo oclusiones parciales y condiciones de iluminación variables. Los primeros estudios se centraron en la generación automática de marcadores con un diseño que asegurara su unicidad y facilidad de detección. Estos marcadores están compuestos por un patrón binario rodeado de un borde negro, lo que mejora su visibilidad y robustez en diferentes condiciones de iluminación \cite{aruco_docs}.

    \begin{figure}[h!] 
        \centering 
        \includegraphics[width=0.8\textwidth]{pictures/arucos_ids.png} % Ejemplo de pose de un marcador ArUco siendo detectada 
        \caption{ArUco's and ID's examples} 
        \label{fig} 
    \end{figure}

    \subsubsection{Aplicaciones Comunes}

    Los marcadores ArUco se utilizan en una variedad de aplicaciones, que incluyen la calibración de cámaras, la realidad aumentada, y el control y navegación de robots y drones. Una de las ventajas de usar ArUco es su capacidad para actuar como puntos de referencia en entornos 3D, permitiendo a los sistemas de visión por computadora calcular la pose de la cámara. Según la documentación, \textit{“Markers can be used as 3D landmarks for camera pose estimation”} \cite{aruco_docs_pdf}. Esta característica hace que los marcadores sean esenciales en sistemas de seguimiento y posicionamiento donde la precisión es crítica.

    \begin{figure}
        \centering
        \begin{minipage}{0.3\textwidth}
            \centering
            \includegraphics[width=0.8\textwidth]{pictures/calibration_aruco.png}
            \caption{Camera CAlibration with ArUco}
            \label{fig:imagen1}
        \end{minipage}
        \hfill
        \begin{minipage}{0.3\textwidth}
            \centering
            \includegraphics[width=0.8\textwidth]{pictures/augmented_reality.jpg}
            \caption{Augmented Reality with ArUco}
            \label{fig:imagen2}
        \end{minipage}
        \hfill
        \begin{minipage}{0.3\textwidth}
            \centering
            \includegraphics[width=0.8\textwidth]{pictures/tracking_aruco.jpg}
            \caption{Real Time Tracking with ArUco}
            \label{fig:imagen2}
        \end{minipage}
        \caption{Common applications of ArUco markers.}
    \end{figure}

    \subsubsection{Formatos de Marcadores}

    Los marcadores ArUco están compuestos por un borde negro externo y una región interna que codifica un patrón binario único. Dependiendo del diccionario que se esté utilizando, el número de bits en el marcador varía, lo que afecta la probabilidad de confusión con otros marcadores y la distancia de detección. Una mayor resolución de los marcadores permite que estos se detecten desde distancias más lejanas, pero puede requerir mayor procesamiento \cite{aruco_docs}.

    La biblioteca ArUco también soporta la creación de diccionarios personalizados, lo que permite a los desarrolladores adaptar los marcadores a las necesidades específicas de sus proyectos. \textit{“The design of a dictionary is important since the idea is that their markers should be as different as possible to avoid confusions”} \cite{aruco_docs_pdf}. Esta flexibilidad es especialmente útil en proyectos donde es crucial mantener la unicidad y fiabilidad de la detección de marcadores en entornos complejos.

    \begin{figure}[h!] 
    \centering 
    \includegraphics[width=0.8\textwidth]{pictures/bits_aruco.png} % Ejemplo de marcador ArUco con su formato binario 
    \caption{Bits of a 7x7 ArUco marker} 
    \label{fig} 
    \end{figure}

\subsection{Aruco vs Embedded Aruco}
    Los marcadores ArUco y los marcadores Embedded ArUco (e-ArUco) son tecnologías utilizadas en visión por computadora para tareas de detección y estimación de pose. Aunque ambos comparten una base común en cuanto a su diseño y algoritmos de detección, tienen diferencias importantes que los hacen adecuados para distintas aplicaciones, especialmente en el contexto de operaciones de alta precisión.

    \subsubsection{Diferencias Principales}

    La principal diferencia entre los marcadores ArUco tradicionales y los marcadores Embedded ArUco radica en la optimización de estos últimos para detecciones de alta precisión en un rango amplio de distancias. Según Khazetdinov et al. (2021), \textit{“a new type of fiducial marker called embedded ArUco (e-ArUco) was developed specially for a task of robust marker detection for a wide range of distances”} \cite{khazetdinov2021}. Los e-ArUco están diseñados para mantener su detectabilidad y precisión en escenarios donde los marcadores ArUco estándar podrían no ser tan efectivos, como cuando se requiere una precisión milimétrica en aplicaciones de aterrizaje de UAVs.

    Otra diferencia significativa es que los marcadores e-ArUco están diseñados para mejorar la robustez de la detección, minimizando los errores que podrían surgir por condiciones de iluminación cambiantes y oclusiones parciales. Estos marcadores se basan en los algoritmos de detección de ArUco, lo que permite su implementación sin cambios sustanciales en los sistemas existentes basados en ArUco \cite{khazetdinov2021}.

    \begin{figure}[h!]
        \centering
        \begin{minipage}{0.3\textwidth}
            \centering
            \includegraphics[width=0.65\textwidth]{pictures/pose_1.png}
            \caption{Generación de ArUco Grande / Chico}
            \label{fig:imagen1}
        \end{minipage}
        \hfill
        \begin{minipage}{0.3\textwidth}
            \centering
            \includegraphics[width=0.55\textwidth]{pictures/pose_2.png}
            \caption{Cálculo del centro del ArUco Grande}
            \label{fig:imagen2}
        \end{minipage}
        \hfill
        \begin{minipage}{0.3\textwidth}
            \centering
            \includegraphics[width=0.5\textwidth]{pictures/pose_3.png}
            \caption{Sobreposición del ArUco Chico en el centro del Grande}
            \label{fig:imagen2}
        \end{minipage}
        \caption{Proceso de generación de marcadores e-ArUco.}
    \end{figure}

    \subsubsection{Casos de Uso}

    Los marcadores ArUco tradicionales como se mencionó en la sección anterior, se utilizan comúnmente en aplicaciones de realidad aumentada, estimacion de poses y navegación de robots y drones. Estos marcadores son versátiles y se pueden adaptar a diversas aplicaciones que no requieren una precisión extrema, siendo ideales para proyectos de localización, realidad aumentada, etc.

    Por otro lado, los marcadores Embedded ArUco (e-ArUco) están específicamente diseñados para situaciones donde la precisión es más crítica. Un ejemplo destacado es el uso en el aterrizaje de UAVs (vehículos aéreos no tripulados), donde se necesita una detección precisa y confiable a diferentes distancias. En un estudio realizado por Khazetdinov et al., tuvo como resultado \textit{“an average landing accuracy was 2.03 cm with a standard deviation of 1.53 cm”} al usar marcadores e-ArUco y un algoritmo de aterrizaje implementado en ROS y probado en el simulador Gazebo \cite{khazetdinov2021}. Esta capacidad hace que los e-ArUco sean ideales para entornos donde se requiere una precisión milimétrica, como en operaciones de aterrizaje autónomo de alta precisión.

    

\subsection{Aruco Detection}
    La detección de marcadores ArUco es un proceso esencial en la visión por computadora que permite la identificación y estimación de la pose de los marcadores en imágenes. La siguiente sección detalla los algoritmos de detección, la implementación en OpenCV y los parámetros que afectan la precisión de la detección.

    \subsubsection{Algoritmos de Detección}

    La detección de los marcadores ArUco se basa en algoritmos de visión por computadora que identifican contornos y patrones específicos en las imágenes. Según la documentación de OpenCV, el proceso de detección comienza con la identificación de cuadrados en la imagen y la verificación de si estos contienen un patrón binario válido correspondiente a un marcador ArUco \cite{opencv_docs_aruco}. El algoritmo implementado en OpenCV utiliza la segmentación de contornos y la detección de bordes para encontrar regiones cuadradas que luego se comprueban para determinar si coinciden con los patrones en el diccionario de ArUco.

    Una vez que se identifica un marcador, el algoritmo calcula la pose de la cámara en relación con el marcador utilizando la proyección inversa. Este proceso es particularmente importante en aplicaciones donde se necesita calcular la posición y orientación de la cámara para la navegación y el control de robots y drones.

    \subsubsection{Implementación en OpenCV}

    OpenCV ofrece una implementación robusta para la detección de marcadores ArUco a través del módulo \texttt{cv::aruco}. La función principal para la detección es \texttt{cv::aruco::detectMarkers}, que se encarga de identificar los marcadores en una imagen y devolver sus esquinas y los IDs correspondientes. La documentación de OpenCV resalta que \textit{“the function detects the markers and returns their IDs and corner positions in the image”} \cite{opencv_tutorial_aruco}.

    El siguiente ejemplo muestra cómo usar OpenCV para detectar marcadores ArUco en Python:

    \begin{verbatim} import cv2 import cv2.aruco as aruco

    Cargar la imagen
    image = cv2.imread('image_path.jpg')

    Definir el diccionario de ArUco
    aruco_dict = aruco.Dictionary_get(aruco.DICT_6X6_250)

    Detectar los marcadores
    corners, ids, _ = aruco.detectMarkers(image, aruco_dict)

    Dibujar los marcadores detectados
    if ids is not None: aruco.drawDetectedMarkers(image, corners, ids) \end{verbatim}

    Después de obtener las esquinas y los IDs de los marcadores, es posible utilizar esta información para calcular la pose de la cámara en relación con el/los marcadores detectados. Para calcular el centro de un marcador ArUco, se pueden utilizar las esquinas detectadas y el tamaño del marcador para estimar su posición en la imagen.

    \subsubsection{Parámetros de Precisión}

    La precisión de la detección de marcadores ArUco depende de varios factores, incluidos la calidad de la imagen, el tamaño del marcador, y los parámetros de calibración de la cámara. Según la documentación de OpenCV, la calibración precisa de la cámara es fundamental para minimizar los errores en la estimación de la pose \cite{opencv_docs_aruco}. Entre los parámetros que afectan la detección se encuentran:

    \begin{itemize} 
        \item \textbf{Distorsión de la lente}: La corrección de la distorsión de la lente mejora la precisión de la detección. 
        \item \textbf{Resolución del marcador}: Marcadores con mayor resolución permiten una detección más precisa a distancias más largas, pero requieren más recursos de procesamiento. 
        \item \textbf{Iluminación y contraste}: La detección puede verse afectada por condiciones de iluminación variables, por lo que es importante que la imagen tenga un buen contraste entre el marcador y el fondo. 
    \end{itemize}

    \begin{figure}[h!] 
        \centering 
            \begin{minipage}{0.48\textwidth}
                \includegraphics[width=0.8\textwidth]{pictures/aruco_no_pose_estimation.png} % Ejemplo de marcadores ArUco 
                \caption{ArUco no pose estimation} 
                \label{fig:aruco_no_pose_estimation}
            \end{minipage}
            \begin{minipage}{0.48\textwidth}
                \includegraphics[width=0.8\textwidth]{pictures/aruco_pose_estimation.png} % Ejemplo de marcadores ArUco 
                \caption{ArUco pose estimation} 
                \label{fig:aruco_pose_estimation}
            \end{minipage}
    \end{figure}

\subsection{Comunicación ROS2 - Pixhawk}
    La comunicación entre ROS 2 y Pixhawk se basa en el uso de MAVLink, un protocolo ligero que permite la transferencia bidireccional de datos entre el controlador de vuelo y una computadora auxiliar (companion computer). Esta integración es crucial para enviar comandos de vuelo en tiempo real y monitorizar el estado del dron.
    
        \subsubsection{Protocolo MAVLink}
        MAVLink (Micro Air Vehicle Link) es un protocolo de comunicación de alto rendimiento diseñado para sistemas de vehículos no tripulados. Según la documentación oficial, \textit{“MAVLink is a very lightweight, header-only message marshalling library for micro air vehicles”} \cite{mavlink_docs}. Este protocolo utiliza un sistema basado en paquetes de mensajes que facilita la transmisión de datos entre la estación de control y el vehículo no tripulado.
    
        Los mensajes de MAVLink están estructurados en comandos específicos que permiten controlar varios aspectos del vuelo y la telemetría, incluyendo parámetros como la posición, velocidad, estado de la batería, y comandos de control de vuelo. Cada mensaje está identificado por un ID único, lo que facilita su procesamiento y permite una comunicación eficiente en tiempo real.
    
        \subsubsection{Opciones de Integración de MAVLink en ROS 2}
        Para facilitar la integración de MAVLink con ROS 2, existen varias opciones de comunicación que permiten enviar y recibir datos entre una computadora auxiliar y el controlador de vuelo Pixhawk. Las dos soluciones más populares son MAVROS y Micro XRCE-DDS.
    
            \paragraph{MAVROS}
            es un conjunto de nodos de ROS que actúa como una interfaz entre ROS y MAVLink, permitiendo que ROS se comunique con el controlador de vuelo mediante topics y servicios. MAVROS permite la publicación y suscripción de mensajes de MAVLink a través de topics en ROS, facilitando la ejecución de comandos de vuelo, obtención de datos de telemetría y control de misiones.
    
            MAVROS implementa una serie de topics predefinidos en ROS, tales como:
            \begin{itemize}
                \item \texttt{/mavros/setpoint\_position/local}: Para establecer puntos de referencia de posición.
                \item \texttt{/mavros/state}: Proporciona el estado actual del dron, incluyendo el modo de vuelo y si el vehículo está armado.
                \item \texttt{/mavros/imu/data}: Datos de IMU para el monitoreo de la orientación y aceleración.
                \item \texttt{/mavros/battery}: Muestra datos de la batería como porcentaje de carga y voltaje.
            \end{itemize}
            
            Con MAVROS, los desarrolladores pueden enviar comandos de control, como armar el dron, cambiar el modo de vuelo, o establecer puntos de referencia para la navegación autónoma. Esto se realiza publicando mensajes en los topics adecuados y ajustando parámetros en tiempo real mediante la interfaz de MAVLink \cite{px4_ros2}.
            
            \begin{figure}
                \centering
                %\includegraphics[width=0.8\textwidth]{pictures/mavros.png} 
                %\caption{Comunicación ROS 2 - Pixhawk con MAVROS.}
                \label{fig:mavros}
            \end{figure}

            \paragraph{Micro XRCE-DDS}
            es una implementación ligera del DDS (Data Distribution Service) que permite la comunicación entre ROS 2 y el controlador de vuelo de forma eficiente en sistemas con limitaciones de recursos. Esta opción es útil en contextos donde el tamaño y rendimiento del sistema son cruciales, como en drones pequeños o con hardware de procesamiento limitado.
    
            Con Micro XRCE-DDS, ROS 2 puede comunicarse directamente con PX4 utilizando el middleware DDS. Esto permite la creación de un sistema distribuido y escalable, en el cual los nodos de ROS 2 pueden enviar y recibir mensajes MAVLink a través de topics DDS, de manera similar a MAVROS, pero con una carga de procesamiento optimizada \cite{px4_ros2}.
    
            \begin{figure}
                \centering
                \includegraphics[width=0.8\textwidth]{pictures/xrce_dds.png} 
                \caption{Comunicación ROS 2 - Pixhawk con Micro XRCE-DDS.}
                \label{fig:xrce_dds}
            \end{figure}
    
\subsection{Sensores para la Navegación en Ambientes Exteriores e Interiores}
    Para lograr el control preciso de la posición del dron mediante los topics de ROS 2 como \texttt{/mavros/setpoint\_position/local}, es necesario contar con sensores que permitan obtener datos de posicionamiento confiables en función del entorno en el que opere el dron. A continuación, se describen los sensores comúnmente utilizados en ambientes exteriores e interiores para proporcionar información de posición.
    
    \subsubsection{Sensores para Ambientes Exteriores}
    En entornos exteriores, los drones suelen utilizar sensores de posicionamiento global (GPS) para determinar su ubicación en tiempo real. A continuación se presentan los sensores más comunes en exteriores:
        \begin{itemize}
            \item \textbf{GPS (Sistema de Posicionamiento Global)}: El GPS es el sensor principal para la navegación en exteriores. Proporciona coordenadas de latitud, longitud y altitud que permiten al dron determinar su posición global. En ROS 2, estos datos se suelen publicar en topics como \texttt{/mavros/global\_position/global} y se integran en los sistemas de control de posición.
            
            \item \textbf{RTK-GPS (Real-Time Kinematic GPS)}: Para aplicaciones de alta precisión, como el aterrizaje de precisión o la navegación en áreas restringidas, se utiliza RTK-GPS. Este sistema mejora la precisión del GPS mediante la corrección de datos en tiempo real, alcanzando precisiones de centímetros. RTK-GPS es compatible con MAVLink y permite un control de posición altamente preciso en ROS 2.
            
            \item \textbf{IMU (Unidad de Medición Inercial)}: Aunque no proporciona datos de ubicación directa, la IMU complementa al GPS detectando cambios en orientación y aceleración. Estos datos son cruciales para mantener la estabilidad y proporcionar información sobre la orientación del dron durante el vuelo.
        \end{itemize}
    
    \subsubsection{Sensores para Ambientes Interiores}
    En ambientes interiores, donde las señales de GPS suelen ser débiles o inexistentes, se emplean otros sensores para determinar la posición y navegación del dron. Los sensores utilizados en interiores incluyen:
        \begin{itemize}
            \item \textbf{Cámaras de Visión (Monocular o Estéreo)}: Las cámaras de visión permiten la localización visual mediante algoritmos de SLAM (Simultaneous Localization and Mapping) o detección de marcadores, como ArUco, para calcular la posición relativa del dron en interiores. Con ROS 2, estas cámaras pueden integrarse con librerías como OpenCV y utilizar los topics de imagen para el procesamiento en tiempo real.
            
            \item \textbf{LIDAR (Light Detection and Ranging)}: Los sensores LIDAR emiten pulsos de luz para medir distancias y obtener mapas detallados del entorno. Estos sensores son útiles para la navegación y el mapeo en tiempo real en interiores y se integran en ROS 2 mediante topics de nube de puntos. Son particularmente efectivos para evitar obstáculos y realizar posicionamiento preciso en interiores.
    
            \item \textbf{Sensores UWB (Ultra-Wideband)}: Los sistemas UWB utilizan señales de radio para calcular distancias precisas entre el dron y puntos de referencia (beacons) fijos. Estos sensores son útiles en áreas cerradas, proporcionando precisión en el rango de centímetros, y permiten al dron calcular su posición relativa sin depender de la visión.
    
            \item \textbf{IMU (Unidad de Medición Inercial)}: En interiores, la IMU sigue siendo un componente esencial para detectar cambios de orientación y movimiento. La fusión de datos de la IMU con otros sensores, como LIDAR o cámaras, ayuda a mantener la estabilidad y proporciona estimaciones de posición precisas mediante filtros de fusión, como el filtro de Kalman.
        \end{itemize}
    
    \subsubsection{Fusión de Sensores para Precisión en Navegación}
    Para mejorar la precisión y estabilidad del control de posición, se recomienda la fusión de datos de múltiples sensores, un proceso que combina la información de cada sensor para obtener una estimación más confiable de la posición y orientación del dron. La fusión de sensores se implementa comúnmente en ROS 2 utilizando filtros de Kalman o técnicas avanzadas de estimación de estados, permitiendo el uso eficiente de los topics de setpoint de posición en diferentes entornos.
    
    Estos sensores permiten al dron obtener datos de posición en tiempo real y de alta precisión, ya sea en exteriores o interiores, facilitando así el uso de los topics de posicionamiento en ROS 2 para control de navegación autónoma.
    
\subsection{Fusión de Sensores usando el Filtro de Kalman en Pixhawk}
    La fusión de sensores es un proceso crucial en la navegación de drones, permitiendo integrar datos de múltiples sensores para obtener una estimación de posición y orientación más precisa. En el Pixhawk, este proceso se realiza mediante el filtro de Kalman extendido (EKF), el cual combina datos de sensores como GPS e IMU en exteriores, y cámaras, LIDAR o UWB e IMU en interiores. A continuación, se detallan los requisitos y configuraciones necesarias para que el sistema de fusión de sensores funcione adecuadamente en cada tipo de entorno.
    
    \subsubsection{Fusión de GPS e IMU en Exteriores}
    En entornos exteriores, el GPS es el sensor principal para la localización global, mientras que la IMU proporciona datos de orientación y aceleración. La fusión de estos sensores en Pixhawk se logra mediante el uso del EKF, que mejora la precisión y confiabilidad de la navegación al compensar posibles errores en las lecturas del GPS y la IMU.
    
    \begin{itemize}
        \item \textbf{Configuración de Pixhawk para GPS e IMU}: Para activar la fusión de GPS e IMU en Pixhawk, es necesario habilitar el EKF y configurar parámetros específicos, tales como:
            \begin{itemize}
                \item \texttt{EKF2\_GPS\_CHECK}: Habilita la verificación de la calidad de la señal de GPS, garantizando que solo se utilicen datos de GPS cuando la señal sea suficiente.
                \item \texttt{EKF2\_HGT\_MODE}: Establece la fuente de altitud para el filtro de Kalman. En exteriores, se recomienda configurarlo para usar datos de GPS o de barómetro.
                \item \texttt{EKF2\_AID\_MASK}: Activa el soporte para sensores adicionales. Para la fusión de GPS e IMU, este parámetro debe incluir \textit{GPS} y \textit{IMU}.
            \end{itemize}
            
        \item \textbf{Requisitos de Precisión de GPS e IMU}: 
            \begin{itemize}
                \item **GPS**: Es ideal contar con un GPS de alta precisión o un sistema RTK-GPS en aplicaciones donde se requiera una precisión de centímetros.
                \item **IMU**: Es importante que la IMU esté correctamente calibrada y que el Pixhawk esté montado en una estructura estable para reducir el ruido de las lecturas.
            \end{itemize}
            
        \item \textbf{Proceso de Fusión con EKF}: 
            El EKF en Pixhawk realiza un cálculo continuo de la posición y velocidad del dron al combinar los datos de GPS e IMU. El GPS proporciona la ubicación global (latitud, longitud y altitud), mientras que la IMU contribuye con datos de orientación y aceleración. El EKF utiliza estos datos para reducir el error acumulativo y generar una estimación de posición precisa y estable.
    \end{itemize}
    
    \subsubsection{Desafíos en la Fusión de Sensores en Ambientes Dinámicos}
    La fusión de sensores en Pixhawk enfrenta desafíos en entornos dinámicos o con interferencias, como en interiores con superficies reflectantes para LIDAR o en exteriores con pérdida de señal GPS. Para minimizar estos problemas, es importante realizar calibraciones periódicas de todos los sensores y ajustar los parámetros del EKF según el entorno y las condiciones de vuelo.
    
    Mediante la fusión de sensores con el EKF en Pixhawk, los drones pueden operar de forma autónoma y precisa tanto en exteriores como en interiores, adaptándose a las condiciones y requerimientos específicos de cada entorno.
    

    \subsubsection{Fusión de Sensores en Interiores sin GPS}
    Para lograr la navegación autónoma en interiores, donde el GPS no está disponible, se emplean sensores alternativos que permiten al dron estimar su posición relativa y mantener un vuelo estable. La fusión de estos sensores se realiza mediante el filtro de Kalman extendido (EKF) en el Pixhawk, que combina datos de diferentes sensores para obtener una estimación precisa de posición y orientación. A continuación se describen los sistemas de posicionamiento más destacados y los requisitos para lograr una fusión de sensores efectiva en interiores.
    
    \subsubsection{Sensores Alternativos para Navegación en Interiores}
    En entornos cerrados, el uso de tecnologías como cámaras, beacons, y sistemas de seguimiento óptico permite al dron obtener información de posición sin depender del GPS. Estos sistemas requieren que se defina manualmente el origen de la posición inicial, usando una estación de control en tierra (GCS) como Mission Planner o mediante scripts en Lua. Algunos de los sensores más destacados incluyen:
    
    \begin{itemize}
        \item \textbf{Intel RealSense T265 y Luxonis OAK-D}: Ambas son cámaras de visión estéreo que pueden proporcionar estimaciones de posición mediante técnicas de SLAM (Simultaneous Localization and Mapping). Estas cámaras permiten calcular la posición y la orientación del dron en tiempo real, y pueden integrarse en el Pixhawk usando el EKF para fusionar estos datos con la IMU. La configuración requiere calibrar correctamente las cámaras y establecer el origen de posición.
    
        \item \textbf{MarvelMind Beacons y Pozyx Beacons}: Estos sistemas basados en balizas (beacons) usan señales de radio para determinar la posición relativa del dron. Los beacons se instalan en ubicaciones fijas dentro del entorno, y el dron calcula su posición en función de la distancia a cada beacon. Este tipo de sistema es muy preciso y puede proporcionar una estimación de posición estable, aunque requiere de una infraestructura inicial de beacons correctamente ubicados en el área.
    
        \item \textbf{ModalAI VOXL y Nokov Indoor Optical Tracking}: Estos sistemas combinan visión por computadora y algoritmos de seguimiento óptico para proporcionar datos de posicionamiento precisos en interiores. ModalAI VOXL, por ejemplo, es un módulo que integra una computadora de alto rendimiento con cámaras de visión para permitir una navegación autónoma sin GPS. El Pixhawk puede fusionar estos datos de posición y orientación en tiempo real mediante el EKF, mejorando la estabilidad del vuelo en interiores.
    
        \item \textbf{Vicon Positioning System y OptiTrack Motion Capture System}: Estos sistemas de captura de movimiento utilizan cámaras de alta precisión colocadas en el entorno para rastrear marcadores ubicados en el dron. Estos sistemas son muy efectivos para aplicaciones de navegación de alta precisión, como en laboratorios o áreas de prueba, y permiten obtener una posición con precisión milimétrica. Sin embargo, requieren un entorno controlado y la instalación de múltiples cámaras en el área.
    
        \item \textbf{Optical Flow}: Este sensor detecta el movimiento relativo entre el dron y la superficie debajo de él. Aunque no proporciona una posición absoluta, el flujo óptico es útil para mantener la estabilidad del dron en interiores y para pequeñas correcciones de posición. La combinación de datos de flujo óptico con la IMU permite una navegación precisa a nivel local.
    \end{itemize}
    
    \subsubsection{Requisitos para la Fusión de Sensores sin GPS}
    La fusión de sensores sin GPS en interiores requiere de un entorno controlado donde estos sistemas de posicionamiento puedan operar de manera efectiva. Los siguientes son algunos de los requisitos necesarios para configurar y utilizar estos sensores en Pixhawk:
    
    \begin{itemize}
        \item \textbf{Definir el Origen de la Posición}: Al utilizar sensores de visión, beacons o sistemas de captura de movimiento, el origen de la posición debe definirse manualmente a través del GCS o con un script en Lua para que el dron pueda interpretar correctamente su posición en relación con el entorno.
    
        \item \textbf{Calibración de Sensores de Visión y Beacons}: La precisión de los datos de posición depende de una calibración adecuada de los sistemas de visión y beacons. Esto incluye la alineación de cámaras y beacons, así como la configuración de parámetros en el EKF para optimizar la precisión de los datos.
    
        \item \textbf{Calidad de IMU}: La IMU es fundamental para la fusión de sensores en interiores, ya que proporciona datos de orientación y aceleración que complementan los datos de posicionamiento relativo. Las IMUs de bajo costo pueden experimentar un alto nivel de deriva, por lo que es recomendable utilizar IMUs de mayor calidad o realizar una calibración frecuente.
    
        \item \textbf{Uso del Filtro de Kalman (EKF) en Pixhawk}: El EKF en Pixhawk debe configurarse para aceptar las entradas de los sensores seleccionados. Esto incluye activar los parámetros necesarios en el Pixhawk, tales como \texttt{EKF2\_AID\_MASK} para habilitar \textit{VISION\_POSITION}, \textit{RANGE\_FINDER}, o \textit{FLOW} dependiendo del sensor utilizado, y ajustar los valores de ruido según las especificaciones del sistema de posicionamiento.
    \end{itemize}
    
    Estos sensores y configuraciones permiten que el dron mantenga un vuelo autónomo y preciso en entornos interiores sin GPS, proporcionando alternativas para aplicaciones de navegación autónoma en espacios cerrados.
    

   





